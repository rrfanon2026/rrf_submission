# Random Rule Forests (RRF)

This repository implements **Random Rule Forests (RRF)**, an interpretable ensemble framework for predicting startup success based on YES/NO questions generated by large language models (LLMs). It includes pipelines for data preparation, question generation, semantic filtering, statistical testing, and precision/recall evaluation.

## Repository Structure
```
.
â”œâ”€â”€ data/                 # anonymised founder-level input data
â”œâ”€â”€ notebooks/            # Jupyter notebooks for generating paper figures
â”œâ”€â”€ precomputed/          # all artefacts needed for reproduction
â”‚   â””â”€â”€ gpt_4o_mini/      # model-specific subfolder (see README for details)
â”œâ”€â”€ scripts/              # numbered pipeline scripts
â”œâ”€â”€ src/                  # core RRF library (question logic, utils, scoring)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # project overview

### OpenAI API Key

Some scripts perform live LLM calls.  
Set your key before running those stages:

```bash
export OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```
(Replace the placeholder with your actual key. The quick-start path that uses precomputed/ artefacts does not require an API key.)


## Key Scripts (live-API stages)

> **Note** â€“ any script that makes LLM calls expects  
> `export OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"`  
> to be set in your shell.

## Key Scripts (live-API stages)

| Script | What it does | Example invocation |
|--------|--------------|--------------------|
| `02_question_generation.py` | Generates **250 candidate YES/NO questions** (10 Ã— 25 files) from an LLM. | `python 02_question_generation.py --provider openai --model gpt-4o-mini` |
| `03_test_semantic_similarity.py` | Removes near-duplicate questions via sentence-transformer embeddings (GPU / high-RAM recommended). | `python 03_test_semantic_similarity.py --model openai --threshold 0.9` |
| `03b_combine_deduplicated_questions.py` | Merges deduplicated questions and repartitions into *N* sets. | `python 03b_combine_deduplicated_questions.py --model gpt-4o-mini` |
| `04_validate_questions.py` | Scores each question on **500 validation founders**. | `python 04_validate_questions.py --provider openai --model gpt-4o-mini --question-set 0` |
| `05_stats_test_questions.py` | Exploratory permutation tests & descriptive stats. | `python 05_stats_test_questions.py` |
| `05b_split_questions.py` | Splits surviving questions into *k* chunks for parallel inference. | `python 05b_split_questions.py --model gpt-4o-mini --n_chunks 7` |
| `06_test_questions.py` | Generates predictions. Modes: `vanilla_few_shot`, `vanilla`, or `questions`. | `python 06_test_questions.py --provider openai --model gpt-4o-mini --mode questions --question_set 0` |

> **Batch runs (optional)**  
> Validate all 10 sets and infer over 7 chunks:

```bash
# 04 â€“ validation (10 sets)
for i in {0..9}; do
  python 04_validate_questions.py --provider openai --model gpt-4o-mini --question-set "$i"
done

# 06 â€“ baseline modes
python 06_test_questions.py --provider openai --model gpt-4o-mini --mode vanilla_few_shot
python 06_test_questions.py --provider openai --model gpt-4o-mini --mode vanilla

# 06 â€“ question-based model, 7 chunks
for i in {0..6}; do
  python 06_test_questions.py --provider openai --model gpt-4o-mini --mode questions --question_set "$i"
done
```

## ðŸ“Š Paper Figures

The `notebooks/paper_figures.ipynb` notebook recreates the core figures from the RRF paper submission using outputs stored in `precomputed/`. This includes:

- Ensemble performance vs. vanilla GPT prompting
- Comparison of LLM-only, LLM + expert collaboration, and expert-only models
- Ablation studies on filtering and ranking strategies
- Comparison of optimisation cost functions (e.g., precision vs. F0.5 vs. F1)

No external API access is required to run this notebook.