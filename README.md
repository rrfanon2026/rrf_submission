# Random Rule Forests (RRF)

This repository implements **Random Rule Forests (RRF)**, an interpretable ensemble framework for predicting startup success based on YES/NO questions generated by large language models (LLMs). It includes pipelines for data preparation, question generation, semantic filtering, statistical testing, and precision/recall evaluation.

## Repository Structure
```
.
├── precomputed/          # all artefacts needed for reproduction
│   └── gpt_4o_mini/      # model-specific subfolder (see README for details)
├── scripts/              # numbered pipeline scripts
├── src/                  # core RRF library (question logic, utils, scoring)
├──  requirements.txt     # Python dependencies
└── README.md             # project overview

### OpenAI API Key

Some scripts perform live LLM calls.  
Set your key before running those stages:

```bash
export OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```
(Replace the placeholder with your actual key. The quick-start path that uses precomputed/ artefacts does not require an API key.)


## Key Scripts (live-API stages)

> **Note** – any script that makes LLM calls expects  
> `export OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"`  
> to be set in your shell.

## Key Scripts (live-API stages)

| Script | What it does | Example invocation |
|--------|--------------|--------------------|
| `02_question_generation.py` | Generates **250 candidate YES/NO questions** (10 × 25 files) from an LLM. | `python 02_question_generation.py --provider openai --model gpt-4o-mini` |
| `03_test_semantic_similarity.py` | Removes near-duplicate questions via sentence-transformer embeddings (GPU / high-RAM recommended). | `python 03_test_semantic_similarity.py --model openai --threshold 0.9` |
| `03b_combine_deduplicated_questions.py` | Merges deduplicated questions and repartitions into *N* sets. | `python 03b_combine_deduplicated_questions.py --model gpt-4o-mini` |
| `04_validate_questions.py` | Scores each question on **500 validation founders**. | `python 04_validate_questions.py --provider openai --model gpt-4o-mini --question-set 0` |
| `05_stats_test_questions.py` | Exploratory permutation tests & descriptive stats. | `python 05_stats_test_questions.py` |
| `05b_split_questions.py` | Splits surviving questions into *k* chunks for parallel inference. | `python 05b_split_questions.py --model gpt-4o-mini --n_chunks 7` |
| `06_test_questions.py` | Generates predictions. Modes: `vanilla_few_shot`, `vanilla`, or `questions`. | `python 06_test_questions.py --provider openai --model gpt-4o-mini --mode questions --question_set 0` |

> **Batch runs (optional)**  
> Validate all 10 sets and infer over 7 chunks:

```bash
# 04 – validation (10 sets)
for i in {0..9}; do
  python 04_validate_questions.py --provider openai --model gpt-4o-mini --question-set "$i"
done

# 06 – baseline modes
python 06_test_questions.py --provider openai --model gpt-4o-mini --mode vanilla_few_shot
python 06_test_questions.py --provider openai --model gpt-4o-mini --mode vanilla

# 06 – question-based model, 7 chunks
for i in {0..6}; do
  python 06_test_questions.py --provider openai --model gpt-4o-mini --mode questions --question_set "$i"
done
